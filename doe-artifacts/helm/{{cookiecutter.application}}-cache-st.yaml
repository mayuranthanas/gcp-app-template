# Default values for helm-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
applicationName: {{cookiecutter.application}}-referencedata-cachemanager
environment: np
replicaCount: 1
namespace: {{cookiecutter.namespace}}

type: deployment

#containerPort: 8090

#set to true if deploying to openshift and want a route created
openshift:
  enabled: false

#TELUS Labels/Annotations
telus:
  cmdbId: "{{cookiecutter.cmdbId}}"
  costCentre: "{{cookiecutter.costCentre}}"
  organization: "{{cookiecutter.organization}}"
  mailingList: "{{cookiecutter.mailingList}}"

volumeMounts:
  - name: secretconfig
    path: /etc/secret

# Allows you set arbitrary env variables.
extraEnvs:
  - name: PROJECT_ID
    value: {{cookiecutter.gcpProjectIdNp}}
  - name: NODE_ENV
    value: development
  - name: CLIENT_APP_NAME
    value: CLOUD_REFPDS_INIT
  - name: GOOGLE_APPLICATION_CREDENTIALS
    value: /etc/secret/serviceAccountKey.json
  - name: REDIS_CONFIG
    value: /etc/secret/redis-refdata-np.json
  - name: LOG_LEVEL
    value: debug

envFrom: []
# envFrom:
#   - secretRef:
#       name: test-secret
#   - configMapRef:
#       name: special-config

# Allows you to add any config files and specify a path location in the volumeMounts
# such as application.yaml or log4j2.properties.  Since this is clear in src control
# please do no use for sensitive data such as keys and passwords.  See Secrets.
# configmaps:
#   file:
#     .env: |
#       REDIS_CONFIG=/etc/secret/redis-refdata-np.json


livenessProbe:
  httpGet:
    path: /
    port: {{cookiecutter.cache.port}}

readinessProbe:
  httpGet:
    path: /
    port: {{cookiecutter.cache.port}}


# Volume mounts should have an equivelent volume name defined of required type
# this example creates and emptyDir type volume for ephemeral data
volumes: |
  - name: secretconfig
    emptyDir: {}
#   - name: test-volume
    # This GCE PD must already exist.
#     gcePersistentDisk:
#      pdName: my-data-disk
#      fsType: ext4

# Mount precreated secrets to a volume, it is determined the helm chart should not create secrets
# to protect security.  Please do not store secret data such as keys, keystores or passwords in Git
# or in a configmap
secretMounts: []
#secretMounts:
#  - name: keystores
#    secretName: app-keystores
#    path: /etc/secrets/keystores

image:
# Based on what you put in cloud-build yaml
# e.g. "gcr.io/${_REPO_ID}/${_NAMESPACE}/${_SERVICE_NAME}:$TAG_NAME"
# _SERVICE_NAME = referencedata-cachemanager
  repository: {{cookiecutter.cache.image}}
  tag: latest
  pullPolicy: Always

service:
 type: ClusterIP
 port: {{cookiecutter.cache.port}}

ingress:
  enabled: false
#  path: /*
#  hosts:
#    - "cio-referencedata-private-yul-np-001"
#  tls: []
#  #  - secretName: chart-example-tls
#  #    hosts:
#  #      - chart-example.local
#
#  class: nginx-ingress-protected
#
# # Can disable modsecurity rule 949110 via annotation below to prevent 403 errors common in either of the following cases:
# #    - You wish to allow PUT/DELETE HTTP methods
# #    - Consumers must make repeated requests, to prevent "Inbound Anomaly Score Exceeded" (ie React UIs).
#
# #annotations:
# #nginx.ingress.kubernetes.io/modsecurity-snippet: SecRuleRemoveByID 949110
#
#
## enable on kong ingress and add plugins
## insure the hostname used is cname to the kong ingress A record.
#  # class: kong
#  # kongPlugins: key-auth
#    
## create required kong plugins
## plugins are reusable within namespace
#kong:
#  enabled: false
#  plugins:
#    - name: key-auth
#      plugin: key-auth
#      config: 
#        key_names: apikey
#  #   - name: acl
#  #     plugin: acl
#  #     config:
#  #       name: test
#  # consumers:
#  #   - name: testconsumer
#  #     username: test
#  # credentials:
#  #   - name: testcredential
#  #     consumer: testconsumer
#  #     type: key-auth
## add any additional credential config as required by type. 
## key for key-auth is autogenerated and can be seen via kubectl get kongcredential...
#      # config:
#      #   key: value
## Add custom kong ingress settings
#  ingress:
#    upstream:
#  #     healthchecks:
#  #       passive:
#  #         healthy:
#  #           http_statuses:
#  #           - 200
#  #           successes: 0
#  #         unhealthy:
#  #           http_failures: 0
#  #           http_statuses:
#  #           - 429
#  #           - 503
#  #           tcp_failures: 0
#  #           timeouts: 0
#    proxy:
#  #     protocol: http
#  #     path: /
#  #     connect_timeout: 10000
#  #     retries: 10
#  #     read_timeout: 10000
#  #     write_timeout: 10000
#    route:
#  #     methods:
#  #     - POST
#  #     - GET
#  #     regex_priority: 0
#  #     strip_path: false
#  #     preserve_host: true
#  #     protocols:
#  #     - http
#  #     - https


resources: {}
#  limits:
#    cpu: 2
#    memory: 3000Mi
#  requests:
#    cpu: 900m
#    memory: 1000Mi
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
  #  memory: 128Mi

# enable autoscaling and set utilization targets.  For autoscaling on cpu/memeory to work you
# must set resource limits
autoscaling:
  enabled: false
#  minReplicas: 2
#  maxReplicas: 5
#  targetCPUUtilizationPercentage: 50
#  targetMemoryUtilizationPercentage: 75

nodeSelector: {}

tolerations: []

affinity: {}


#This sidecar implementation would effectively allow you to access your instance host with 'localhost'
#Uncomment the snippet of values below to implement CloudSQL proxy for your implementation

#cloudsqlproxy:
#  enabled: false
#  version: "1.16"
#  instance_connection: cio-referencedata-np-faf2a2:northamerica-northeast1:**

# sidecar: |
#   - name: cloudsql-proxy
#     image: gcr.io/cloudsql-docker/gce-proxy:1.14
#     command: ["/cloud_sql_proxy",
#     "-instances=INSTANCE_CONNECTION_NAME=tcp:5432"]
#     securityContext:
#       runAsUser: 2  
#       allowPrivilegeEscalation: false    
#
#  replace INSTANCE_CONNECTION_NAME with your cloudsql instance ie cio-cio-lab-eugene-np-0cdac4:northamerica-northeast1:genesys-3b7fae0c

prometheus:
  enable: false
  # scrapePath: "/mypath"
  # scrapePort: 9000

## Important this section configures the GKE side for workload identity support.  Make sure 
## the Terraform side is also completed.  Read more about it at the link below
## https://github.com/telus/tf-module-gcp-workload-identity

initContainers: |
  - name: gcloud-init
    image: google/cloud-sdk:alpine
    volumeMounts:
      - name: secretconfig
        mountPath: /etc/secret
    command:
      - '/bin/sh'
      - '-c'
      - |
        curl -s -H 'Metadata-Flavor: Google' 'http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token' --retry 30 --retry-connrefused --retry-max-time 30 > /dev/null || exit 1;
        echo $(gcloud secrets versions access latest --secret cache-manager-sa-key --project {{cookiecutter.gcpProjectIdNp}}) >> /etc/secret/serviceAccountKey.json;
        echo $(gcloud secrets versions access latest --secret redis-refdata-np --project {{cookiecutter.gcpProjectIdNp}}) >> /etc/secret/redis-refdata-np.json;
rbac:
 create: true
 serviceAccountName: {{cookiecutter.ksa}}   #used in workload_identity module
 serviceAccountAnnotations:                 #sa in with access to firestore and pubsub
   iam.gke.io/gcp-service-account: {{cookiecutter.gsa}}

#Example

# sideCarContainers:
#   datadog:
#     image: datadog/agent:latest
#     env:
#       - name: DD_API_KEY
#         value: ASDF-1234
#       - name: SD_BACKEND
#         value: docker
#sideCarContainers: {}